{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# COMP3411/9418 21T0 Assignment 2\n",
    "\n",
    "- Lecturer: Anna Trofimova\n",
    "- School of Computer Science and Engineering, UNSW Sydney\n",
    "- Last Update 26th January at 07:00am, 2021\n",
    "$$\n",
    "% macros\n",
    "\\newcommand{\\indep}{\\perp \\!\\!\\!\\perp}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student:\n",
    "Xingyu,TAN z5237560"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission\n",
    "This interactive notebook contains the instructions to complete assignment 2; You should submit this notebook with the code and answers in one single file in .ipybn format with the name assignment2.ipybn. **Write your name and zID in the cell above** (to edit the markdown text double-click the cell).\n",
    "\n",
    "There is a maximum file size cap of 5MB, so make sure your submission does not exceed this size. The submitted notebook should contain all your source code and answers. You can add new cells and use markdown text to organise and explain your implementation/answer.\n",
    "\n",
    "Submit your files using give. On a CSE Linux machine, type the following on the command-line:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ give cs3411 ass2 assignment2.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The submission deadline is **3rd February at 11.59pm, 2021.** This is a hard deadline, no extentions will be granted (it is not our decision, it is the reality of the summer term courses)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Late Submission Policy \n",
    "The penalty is set at 20% per late day. This is a ceiling penalty, so if your submission is marked 12/20 and it was submitted two days late, you still get 12/20. If you submit 5 days later, then the penalty is 100% and your mark will be 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plagiarism\n",
    "This is an individual assignment. Remember that **all** work submitted for this assignment must be your own **individual** work and no code\n",
    "sharing or copying is allowed. You may use code from the Internet only with suitable attribution\n",
    "of the source in your program. **Do not use public code repositories as your code might be copied.** Keep in mind that sharing parts of assignment solutions is a form of plagiarism. All submitted assignments will be run through plagiarism detection software to detect similarities to other submissions. You should carefully read the UNSW policy on academic integrity and plagiarism. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "\n",
    "Imagine that you have been hired as a Data Scientist by Amazon, and your first task is to evaluate customer sentiment towards their products. Many online stores selling Amazon products provide their customers with an option to leave a review, but they might not have a rating system, or the customers might choose to leave a review without rating. However, you still want to use these reviews in your report. Thus, you need to develop a reliable model that can automatically assign setiment given a review.\n",
    "\n",
    "To develop your model, you have been given a collection of customer reviews on products like Alexa Echo, Echo dots, Alexa Firesticks etc. with their corresponding ratings. The ratings vary from 1 to 5, but to simplify the problem, you will consider reviews with ratings 1 & 2 to have negative sentiment, with 3 having neutral sentiment,  and 4 & 5 having positive sentiment.\n",
    "* Negative: 1 & 2\n",
    "* Neutral: 3\n",
    "* Positive: 4 & 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "When working with a Jupyter notebook, you can edit the \\*.py files either in the Jupyter interface (in your browser) or with your favorite editor (e.g., PyCharm). Whenever you save a \\*.py file, the notebook will reload their content directly.\n",
    "\n",
    "**Do not create new markdown cells, if you want to comment on something then use Raw NBConvert cells.**\n",
    "\n",
    "Below are the libraries that you can use (and need) in this assignment. If you want to use a library that is not in the list then send us an email to confirm (use course email). \n",
    "\n",
    "Run the code below to import the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import graphviz\n",
    "from graphviz import Source\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import re\n",
    "import copy\n",
    "\n",
    "\n",
    "#  If you experience problems with downloading stopwords, uncomment and run the code below.\n",
    "#  It will launch NLTK Downloader application so you can download stopwords corpora manually.\n",
    "\n",
    "#import nltk\n",
    "# import ssl\n",
    "\n",
    "# try:\n",
    "#     _create_unverified_https_context = ssl._create_unverified_context\n",
    "# except AttributeError:\n",
    "#     pass\n",
    "# else:\n",
    "#     ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Data [2 marks]\n",
    "\n",
    "In this part of the assignment you need to import the Amazon reviews dataset and analyse its main properties.\n",
    "\n",
    "#### Task 1.1\n",
    "Import the dataset from *amazon_alexa.tsv* file, save it into the variable *data* and change the rating labels as follow: \n",
    " {1: 'negative', 2: 'negative', 3: 'neutral', 4: 'positive', 5: 'positive'}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "data = pd.read_csv('amazon_alexa.tsv', sep='\\t')\n",
    "data = data.replace({'rating': {1: 'negative', 2: 'negative', 3: 'neutral', 4: 'positive', 5: 'positive'}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to plot the data distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data['rating'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give a simple description of the class frequency distribution across the full dataset. What do you notice about the distribution? "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "The majority reviews is possitive, then the negative, neutral is the last.\n",
    "the number of positive reviews is 5 times larger than the sum of negative and neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1.2\n",
    "\n",
    "To reduce computation time in this task, your will use only the 1000 most frequent tokens from the vocabulary as attributes. Run the code below to convert the input text samples into a document-term matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>!</th>\n",
       "      <th>!!</th>\n",
       "      <th>&amp;</th>\n",
       "      <th>&amp;#34;Alexa,</th>\n",
       "      <th>&amp;#34;Things</th>\n",
       "      <th>(which</th>\n",
       "      <th>,</th>\n",
       "      <th>-</th>\n",
       "      <th>--</th>\n",
       "      <th>.</th>\n",
       "      <th>...</th>\n",
       "      <th>worth</th>\n",
       "      <th>would</th>\n",
       "      <th>wouldn't</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yet</th>\n",
       "      <th>yet.</th>\n",
       "      <th>you</th>\n",
       "      <th>you're</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   !  !!  &  &#34;Alexa,  &#34;Things  (which  ,  -  --  .  ...  worth  would  \\\n",
       "0  0   0  0            0            0       0  0  0   0  0  ...      0      0   \n",
       "1  0   0  0            0            0       0  0  0   0  0  ...      0      0   \n",
       "2  0   0  0            0            0       0  0  0   0  0  ...      0      0   \n",
       "3  0   0  0            0            0       0  0  0   0  0  ...      0      0   \n",
       "4  0   0  0            0            0       0  0  0   0  0  ...      0      0   \n",
       "\n",
       "   wouldn't  year  years  yet  yet.  you  you're  your  \n",
       "0         0     0      0    0     0    0       0     0  \n",
       "1         0     0      0    0     0    0       0     0  \n",
       "2         0     0      0    0     0    2       0     0  \n",
       "3         0     0      0    0     0    0       0     0  \n",
       "4         0     0      0    0     0    0       0     0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(analyzer=lambda x: x.split(), max_features=1000)\n",
    "\n",
    "x_dense = vectorizer.fit_transform(data['verified_reviews'])\n",
    "x_sparse = pd.DataFrame.sparse.from_spmatrix(x_dense, columns=vectorizer.get_feature_names())\n",
    "#print(x_dense)\n",
    "\n",
    "x_sparse.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was written in the helper notebook that the original input representation is bad. Explain why a document-term matrix is a more suitable representation for machine learning tasks with text data."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "The original input representation can not be operated for every single word, which could help us with machine learning. \n",
    "furthermore, in the original input representation, there is no connection between each review, \n",
    "therefore, the results cannot be inferred through the commonality of the same class of reviews.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two variables *x_dense* and *x_sparce* both represent input data in a document-term form. Explain which one is preferable and why."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x_sparce is better.\n",
    "\n",
    "x_dense: Compared with x_sparce, points with no value are deleted, which has advantages in space and lower space cost during calculation.\n",
    "\n",
    "\n",
    "x_sparce: Compared with x_dense, although there are many tedious and useless values, the whole is embodied in a mathematical matrix, which can be used more in mathematical operations like matrix multiplication.\n",
    "In addition, you can add and modify data during preprocessing in x_sparce.\n",
    "Besides, it has higher calculation operability and suitable for machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Decision Tree [2 marks]\n",
    "\n",
    "In this part you need to evaluate how classifier hyperparameters affect their performance. Consider a Decision Tree classifier that splits attributes based on the lowest entropy. For performance evaluation, assume that only the top 1000 most frequent tokens are used as attributes. The first 60% of samples should constitute the traing set while the remaining 40% of samples should constitute the test set.\n",
    "\n",
    "#### Task 2.1\n",
    "In the cell below place your code that computes performance measures for decision trees with different depth limits. <br> \n",
    "Consider the following cases: max_depth = 5, 10, 100, 200, None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when depth is  5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.07      0.40      0.12        20\n",
      "     neutral       0.01      0.07      0.02        14\n",
      "    positive       0.98      0.87      0.92      1226\n",
      "\n",
      "    accuracy                           0.85      1260\n",
      "   macro avg       0.36      0.45      0.36      1260\n",
      "weighted avg       0.96      0.85      0.90      1260\n",
      "\n",
      "when depth is  10\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.10      0.33      0.16        33\n",
      "     neutral       0.01      0.04      0.02        26\n",
      "    positive       0.97      0.87      0.92      1201\n",
      "\n",
      "    accuracy                           0.84      1260\n",
      "   macro avg       0.36      0.42      0.37      1260\n",
      "weighted avg       0.93      0.84      0.88      1260\n",
      "\n",
      "when depth is  100\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.28      0.27      0.27       111\n",
      "     neutral       0.07      0.08      0.08        62\n",
      "    positive       0.90      0.90      0.90      1087\n",
      "\n",
      "    accuracy                           0.80      1260\n",
      "   macro avg       0.42      0.42      0.42      1260\n",
      "weighted avg       0.80      0.80      0.80      1260\n",
      "\n",
      "when depth is  200\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.22      0.22      0.22       109\n",
      "     neutral       0.06      0.06      0.06        68\n",
      "    positive       0.89      0.89      0.89      1083\n",
      "\n",
      "    accuracy                           0.79      1260\n",
      "   macro avg       0.39      0.39      0.39      1260\n",
      "weighted avg       0.79      0.79      0.79      1260\n",
      "\n",
      "when depth is  None\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.28      0.26      0.27       114\n",
      "     neutral       0.06      0.07      0.06        60\n",
      "    positive       0.90      0.90      0.90      1086\n",
      "\n",
      "    accuracy                           0.80      1260\n",
      "   macro avg       0.41      0.41      0.41      1260\n",
      "weighted avg       0.80      0.80      0.80      1260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rate_value = data['rating']\n",
    "value_train, value_test, rate_train, rate_test = train_test_split(x_sparse, rate_value, test_size = 0.4,shuffle = False)\n",
    "\n",
    "for dep in [5, 10, 100, 200, None]:\n",
    "    print('when depth is ', dep)\n",
    "    classifier= tree.DecisionTreeClassifier(max_depth = dep)\n",
    "    model = classifier.fit(value_train, rate_train)\n",
    "    rate_test_solu = model.predict(value_test)\n",
    "    print(classification_report(rate_test_solu, rate_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2.2\n",
    "In the cell below explain any difference in performance, and comment on metrics in relation to the depth limit.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "1. Regardless of the depth, the accuracy of positive review can always reach more than 90\n",
    "\n",
    "2. As the depth increases, the overall accuracy has decreased. and the accuracy of negative and neutral reviews are improved, but it is still too low compared to the positive. However, the accuracy of positive reviews has declined. \n",
    "\n",
    "3. It is guessed that the main reason is with the increasement of depth, and the better the fitting effect of the decision tree for neutral and negative review.\n",
    "Based on the findings of part1, the sample base of neutral and negative review is not large enough, and the sample number of positive itself is several times higher than that of neutral and negative. So the machine learning effect of positive is better and its accuracy is always high. \n",
    "However, the sample size of the remaining two reviews is still low, so the learning effect and prediction effect are also low.\n",
    "\n",
    "4. At the same time, the overall accuracy has decreased, which may be due to the overfitting of the decision tree.\n",
    "\n",
    "Therefore, in order to have a better machine learning effect, The depth of DT needs to reach an appropriate amount, neither too shallow nor too deep.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 - Pre-processing [3 marks]\n",
    "\n",
    "In this part, you need to evaluate the effect that the pre-processing of input features has on the performance of three classifiers - Decision Tree (DT), Multinomial Naive Bayes (MNB), and  Artificial Neural Networks (ANN). For a Decision Tree classifier, use the parameters from Part2 but do not limit the depth. For a Artificial Neural Network classifier, set the number of hidden neurons to 200.\n",
    "\n",
    "For the performance evaluation, assume that the first 60% of samples constitutes the traing set while the remaining 40% of samples constitutes the test set. All tokens in the vocabulary should be used as attributes.\n",
    "Consider the following scenarios:\n",
    "1. No data pre-processing\n",
    "2. Removal of URLs and tokens that are not made of strings of letters, numbers, or symbols {’, #, @, $} delimited by spaces.\n",
    "3. Apply (2) and make all tokens lowercase.\n",
    "4. Apply (2), (3) and remove all stop words.\n",
    "\n",
    "#### Task 3.1\n",
    "In the cell below place your code that reflects the four scenarios of feature pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# case 1\n",
    "def preprocessing_1(x):\n",
    "    new_return = x.copy(deep=True)\n",
    "    return new_return\n",
    "\n",
    "# case 2\n",
    "def preprocessing_2(x):\n",
    "    x_new = x.copy(deep=True)\n",
    "    for i in range(0,len(x_new['verified_reviews'])):\n",
    "        comment = x_new['verified_reviews'][i]\n",
    "        comment = re.sub(r'^((ht|f)tps?):\\/\\/[\\w\\-]+(\\.[\\w\\-]+)+([\\w\\-.,@?^=%&:\\/~+#]*[\\w\\-@?^=%&\\/~+#])?', '', comment)\n",
    "        comment = re.sub(r'[^a-zA-Z\\d\\'\\#\\@\\$\\s*]', '', comment)\n",
    "        x_new.loc[i,'verified_reviews'] = comment\n",
    "        #print(i)\n",
    "    #print(x_new)\n",
    "    return x_new\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# case 3\n",
    "def preprocessing_3(x):\n",
    "    x_new = preprocessing_2(x)\n",
    "    for i in range(0,len(x_new['verified_reviews'])):\n",
    "        comment = x_new['verified_reviews'][i]\n",
    "        comment = comment.lower()\n",
    "        x_new.loc[i,'verified_reviews'] = comment\n",
    "        #print(i)\n",
    "    #print(x_new)\n",
    "    return x_new\n",
    "\n",
    "\n",
    "# case 4\n",
    "def preprocessing_4(x):\n",
    "    x_new = preprocessing_3(x)\n",
    "    for i in range(0,len(x_new['verified_reviews'])):\n",
    "        comment = x_new['verified_reviews'][i]\n",
    "        comment= comment.split()\n",
    "        comment_sw = [i for i in comment if not i in stopwords.words('english')]\n",
    "        comment = (\" \").join(comment_sw)\n",
    "        x_new.loc[i,'verified_reviews'] = comment\n",
    "\n",
    "    return x_new\n",
    "\n",
    "data_used1 = preprocessing_1(data)\n",
    "data_used2 = preprocessing_2(data)\n",
    "data_used3 = preprocessing_3(data)\n",
    "data_used4 = preprocessing_4(data)\n",
    "combo = [data_used1, data_used2, data_used3, data_used4]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3.2 \n",
    "\n",
    "In the cell below, place your code that trains and tests all three classifiers. Pre-process your data according on scenario #4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution for DT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.38      0.31      0.34       133\n",
      "     neutral       0.06      0.11      0.08        35\n",
      "    positive       0.91      0.90      0.91      1092\n",
      "\n",
      "    accuracy                           0.82      1260\n",
      "   macro avg       0.45      0.44      0.44      1260\n",
      "weighted avg       0.83      0.82      0.82      1260\n",
      "\n",
      "solution for MNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.32      0.62      0.43        56\n",
      "     neutral       0.18      0.20      0.19        60\n",
      "    positive       0.96      0.91      0.93      1144\n",
      "\n",
      "    accuracy                           0.86      1260\n",
      "   macro avg       0.49      0.58      0.52      1260\n",
      "weighted avg       0.89      0.86      0.87      1260\n",
      "\n",
      "solution for ANN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.21      0.42      0.28        55\n",
      "     neutral       0.09      0.18      0.12        34\n",
      "    positive       0.97      0.90      0.93      1171\n",
      "\n",
      "    accuracy                           0.86      1260\n",
      "   macro avg       0.42      0.50      0.44      1260\n",
      "weighted avg       0.91      0.86      0.88      1260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TASK 3.2\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=lambda x: x.split(), max_features=1000)\n",
    "x_dense = vectorizer.fit_transform(data_used4['verified_reviews'])\n",
    "x_sparse = pd.DataFrame.sparse.from_spmatrix(x_dense, columns=vectorizer.get_feature_names())\n",
    "\n",
    "rate_value = data_used4['rating']\n",
    "value_train, value_test, rate_train, rate_test = train_test_split(x_sparse, rate_value, test_size = 0.4, random_state=0, shuffle = False)\n",
    "\n",
    "print('solution for DT')\n",
    "  \n",
    "classifier= tree.DecisionTreeClassifier(max_depth = None)\n",
    "model = classifier.fit(value_train, rate_train)\n",
    "rate_test_solu = model.predict(value_test)\n",
    "print(classification_report(rate_test_solu, rate_test))\n",
    "\n",
    "print('solution for MNB')\n",
    "  \n",
    "classifier= MultinomialNB()\n",
    "model = classifier.fit(value_train, rate_train)\n",
    "rate_test_solu = model.predict(value_test)\n",
    "print(classification_report(rate_test_solu, rate_test))\n",
    "\n",
    "print('solution for ANN')\n",
    "classifier= MLPClassifier(hidden_layer_sizes=(50,50,50,50))\n",
    "model = classifier.fit(value_train, rate_train)\n",
    "rate_test_solu = model.predict(value_test)\n",
    "print(classification_report(rate_test_solu, rate_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution for DT\n",
      "Case1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.24      0.23      0.24       111\n",
      "     neutral       0.06      0.07      0.06        60\n",
      "    positive       0.90      0.89      0.90      1089\n",
      "\n",
      "    accuracy                           0.80      1260\n",
      "   macro avg       0.40      0.40      0.40      1260\n",
      "weighted avg       0.80      0.80      0.80      1260\n",
      "\n",
      "Case2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.31      0.36      0.34        94\n",
      "     neutral       0.09      0.09      0.09        70\n",
      "    positive       0.91      0.90      0.90      1096\n",
      "\n",
      "    accuracy                           0.81      1260\n",
      "   macro avg       0.44      0.45      0.44      1260\n",
      "weighted avg       0.82      0.81      0.81      1260\n",
      "\n",
      "Case3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.41      0.37      0.39       118\n",
      "     neutral       0.10      0.13      0.12        53\n",
      "    positive       0.92      0.91      0.91      1089\n",
      "\n",
      "    accuracy                           0.83      1260\n",
      "   macro avg       0.48      0.47      0.47      1260\n",
      "weighted avg       0.83      0.83      0.83      1260\n",
      "\n",
      "Case4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.35      0.30      0.32       126\n",
      "     neutral       0.09      0.12      0.10        48\n",
      "    positive       0.91      0.90      0.90      1086\n",
      "\n",
      "    accuracy                           0.81      1260\n",
      "   macro avg       0.45      0.44      0.44      1260\n",
      "weighted avg       0.82      0.81      0.82      1260\n",
      "\n",
      "solution for MNB\n",
      "Case1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.39      0.58      0.46        73\n",
      "     neutral       0.07      0.10      0.09        49\n",
      "    positive       0.95      0.91      0.93      1138\n",
      "\n",
      "    accuracy                           0.85      1260\n",
      "   macro avg       0.47      0.53      0.49      1260\n",
      "weighted avg       0.88      0.85      0.87      1260\n",
      "\n",
      "Case2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.36      0.55      0.44        71\n",
      "     neutral       0.13      0.18      0.16        49\n",
      "    positive       0.95      0.91      0.93      1140\n",
      "\n",
      "    accuracy                           0.86      1260\n",
      "   macro avg       0.48      0.55      0.51      1260\n",
      "weighted avg       0.89      0.86      0.87      1260\n",
      "\n",
      "Case3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.37      0.62      0.46        65\n",
      "     neutral       0.15      0.22      0.18        45\n",
      "    positive       0.96      0.91      0.94      1150\n",
      "\n",
      "    accuracy                           0.87      1260\n",
      "   macro avg       0.49      0.58      0.53      1260\n",
      "weighted avg       0.90      0.87      0.88      1260\n",
      "\n",
      "Case4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.32      0.62      0.43        56\n",
      "     neutral       0.18      0.20      0.19        60\n",
      "    positive       0.96      0.91      0.93      1144\n",
      "\n",
      "    accuracy                           0.86      1260\n",
      "   macro avg       0.49      0.58      0.52      1260\n",
      "weighted avg       0.89      0.86      0.87      1260\n",
      "\n",
      "solution for ANN\n",
      "Case1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.28      0.51      0.36        59\n",
      "     neutral       0.04      0.17      0.07        18\n",
      "    positive       0.97      0.89      0.93      1183\n",
      "\n",
      "    accuracy                           0.87      1260\n",
      "   macro avg       0.43      0.52      0.45      1260\n",
      "weighted avg       0.93      0.87      0.89      1260\n",
      "\n",
      "Case2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.24      0.60      0.34        43\n",
      "     neutral       0.15      0.15      0.15        67\n",
      "    positive       0.96      0.91      0.93      1150\n",
      "\n",
      "    accuracy                           0.86      1260\n",
      "   macro avg       0.45      0.55      0.48      1260\n",
      "weighted avg       0.89      0.86      0.87      1260\n",
      "\n",
      "Case3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.36      0.58      0.45        67\n",
      "     neutral       0.07      0.38      0.12        13\n",
      "    positive       0.98      0.90      0.94      1180\n",
      "\n",
      "    accuracy                           0.88      1260\n",
      "   macro avg       0.47      0.62      0.50      1260\n",
      "weighted avg       0.94      0.88      0.90      1260\n",
      "\n",
      "Case4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.24      0.38      0.29        69\n",
      "     neutral       0.10      0.25      0.15        28\n",
      "    positive       0.96      0.89      0.92      1163\n",
      "\n",
      "    accuracy                           0.85      1260\n",
      "   macro avg       0.43      0.51      0.46      1260\n",
      "weighted avg       0.90      0.85      0.87      1260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TASK 3.3\n",
    "\n",
    "\n",
    "print('solution for DT')\n",
    "for i in range(0, 4):\n",
    "    print('Case' + str(i+1))\n",
    "    new_data = combo[i]\n",
    "    vectorizer = CountVectorizer(analyzer=lambda x: x.split(), max_features=1000)\n",
    "    x_dense = vectorizer.fit_transform(new_data['verified_reviews'])\n",
    "    x_sparse = pd.DataFrame.sparse.from_spmatrix(x_dense, columns=vectorizer.get_feature_names())\n",
    "\n",
    "    rate_value = new_data['rating']\n",
    "    value_train, value_test, rate_train, rate_test = train_test_split(x_sparse, rate_value, test_size = 0.4, shuffle = False)\n",
    "    \n",
    "    classifier= tree.DecisionTreeClassifier(max_depth = None)\n",
    "    model = classifier.fit(value_train, rate_train)\n",
    "    rate_test_solu = model.predict(value_test)\n",
    "    print(classification_report(rate_test_solu, rate_test))\n",
    "\n",
    "print('solution for MNB')\n",
    "for i in range(0, 4):\n",
    "    print('Case' + str(i +1))\n",
    "\n",
    "    vectorizer = CountVectorizer(analyzer=lambda x: x.split(), max_features=1000)\n",
    "    x_dense = vectorizer.fit_transform(combo[i]['verified_reviews'])\n",
    "    x_sparse = pd.DataFrame.sparse.from_spmatrix(x_dense, columns=vectorizer.get_feature_names())\n",
    "\n",
    "    rate_value = combo[i]['rating']\n",
    "    value_train, value_test, rate_train, rate_test = train_test_split(x_sparse, rate_value, test_size = 0.4, shuffle = False)\n",
    "    classifier= MultinomialNB()\n",
    "    model = classifier.fit(value_train, rate_train)\n",
    "    rate_test_solu = model.predict(value_test)\n",
    "    print(classification_report(rate_test_solu, rate_test))\n",
    "\n",
    "print('solution for ANN')\n",
    "for i in range(0, 4):\n",
    "    print('Case' + str(i+1))\n",
    "\n",
    "    vectorizer = CountVectorizer(analyzer=lambda x: x.split(), max_features=1000)\n",
    "    x_dense = vectorizer.fit_transform(combo[i]['verified_reviews'])\n",
    "    x_sparse = pd.DataFrame.sparse.from_spmatrix(x_dense, columns=vectorizer.get_feature_names())\n",
    "\n",
    "    rate_value = combo[i]['rating']\n",
    "    value_train, value_test, rate_train, rate_test = train_test_split(x_sparse, rate_value, test_size = 0.4, shuffle = False)\n",
    "    classifier= MLPClassifier(hidden_layer_sizes=(50,50,50,50,))\n",
    "    model = classifier.fit(value_train, rate_train)\n",
    "    rate_test_solu = model.predict(value_test)\n",
    "    print(classification_report(rate_test_solu, rate_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3.3\n",
    "\n",
    "Evaluate the effect each pre-processing scenario has on the performance for the three models. In each case. compute the performance metrics. Write your answer in the cell below. \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. After data preprocessing, the accuracy of DT, MNB, and ANN data testing has been improved.\n",
    "the improvement effect of the decision tree is more obvious, especially neutral and negative reviews.\n",
    "\n",
    "2. preprocessing two is better than preprocessing one, and preprocessing three and four are better than preprocessing two. There is little difference between processing three and preprocessing four.\n",
    "But for the accuracy, the accuracy of preprocessing four is not as good as preprocessing three.\n",
    "\n",
    "In my opinion, there are some important words in stopwords that are omitted.\n",
    "\n",
    "3. Data preprocessing is not very helpful for ANN\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 [5 marks]\n",
    "\n",
    "In this part, you need to evaluate how the number of features and classes affects the performance of the three classifiers - Decision Tree (DT), Multinomial Naive Bayes (MNB), and Artificial Neural Networks (ANN). For a Decision Tree classifier, use the parameters from Part 2 but do not limit the depth. For a Artificial Neural Network classifier set the number of hidden neurons to 200.\n",
    "\n",
    "For the performance evaluation, assume that the first 60% of samples constitute the traing set while the remaining 40% of samples constitute the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4.1\n",
    "\n",
    "Consider the following scenarios for attributes:\n",
    "\n",
    "1. Attributes are all tokens in the vocabulary\n",
    "2. Attributes are the top 1000 most frequent tokens in the vocabulary\n",
    "3. Attributes are the top 100 most frequent tokens in the vocabulary\n",
    "4. Attributes are the top 10 most frequent tokens in the vocabulary\n",
    "\n",
    "In the cell below, place your code that returns a document-term representation given the number of attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "\n",
    "def limit_attributes(x, n_attributes):\n",
    "    vectorizer = CountVectorizer(analyzer=lambda x: x.split(), max_features=n_attributes)\n",
    "    x_dense = vectorizer.fit_transform(x['verified_reviews'])\n",
    "    x_sparse = pd.DataFrame.sparse.from_spmatrix(x_dense, columns=vectorizer.get_feature_names())\n",
    "    return x_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4.2 \n",
    "\n",
    "In the cell below, place your code that trains and tests all three classifiers. Limit the number of attributes according on scenario #4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution for DT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.15      0.18      0.16        91\n",
      "     neutral       0.09      0.11      0.10        54\n",
      "    positive       0.90      0.88      0.89      1115\n",
      "\n",
      "    accuracy                           0.79      1260\n",
      "   macro avg       0.38      0.39      0.38      1260\n",
      "weighted avg       0.81      0.79      0.80      1260\n",
      "\n",
      "solution for MNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.02      1.00      0.04         2\n",
      "     neutral       0.00      0.00      0.00         1\n",
      "    positive       1.00      0.86      0.93      1257\n",
      "\n",
      "    accuracy                           0.86      1260\n",
      "   macro avg       0.34      0.62      0.32      1260\n",
      "weighted avg       1.00      0.86      0.92      1260\n",
      "\n",
      "solution for ANN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.16      0.21      0.18        81\n",
      "     neutral       0.12      0.24      0.16        34\n",
      "    positive       0.93      0.88      0.90      1145\n",
      "\n",
      "    accuracy                           0.82      1260\n",
      "   macro avg       0.40      0.44      0.41      1260\n",
      "weighted avg       0.86      0.82      0.84      1260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print('solution for DT')\n",
    "\n",
    "x_sparse = limit_attributes(data, 10)\n",
    "\n",
    "rate_value = data['rating']\n",
    "value_train, value_test, rate_train, rate_test = train_test_split(x_sparse, rate_value, test_size = 0.4, random_state=1, shuffle = False)\n",
    "\n",
    "classifier= tree.DecisionTreeClassifier(max_depth = None)\n",
    "model = classifier.fit(value_train, rate_train)\n",
    "rate_test_solu = model.predict(value_test)\n",
    "print(classification_report(rate_test_solu, rate_test))\n",
    "\n",
    "print('solution for MNB')\n",
    "\n",
    "x_sparse = limit_attributes(data, 10)\n",
    "\n",
    "rate_value = data['rating']\n",
    "value_train, value_test, rate_train, rate_test = train_test_split(x_sparse, rate_value, test_size = 0.4, random_state=1, shuffle = False)\n",
    "classifier= MultinomialNB()\n",
    "model = classifier.fit(value_train, rate_train)\n",
    "rate_test_solu = model.predict(value_test)\n",
    "print(classification_report(rate_test_solu, rate_test))\n",
    "\n",
    "print('solution for ANN')\n",
    "\n",
    "x_sparse = limit_attributes(data, 10)\n",
    "\n",
    "rate_value = data['rating']\n",
    "value_train, value_test, rate_train, rate_test = train_test_split(x_sparse, rate_value, test_size = 0.4, random_state=1, shuffle = False)\n",
    "classifier= MLPClassifier(hidden_layer_sizes=(50,50,50,50))\n",
    "model = classifier.fit(value_train, rate_train)\n",
    "rate_test_solu = model.predict(value_test)\n",
    "print(classification_report(rate_test_solu, rate_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution for DT\n",
      "Case1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.18      0.45      0.25        42\n",
      "     neutral       0.06      0.09      0.07        43\n",
      "    positive       0.95      0.88      0.92      1175\n",
      "\n",
      "    accuracy                           0.84      1260\n",
      "   macro avg       0.40      0.48      0.41      1260\n",
      "weighted avg       0.90      0.84      0.86      1260\n",
      "\n",
      "Case2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.23      0.23      0.23       110\n",
      "     neutral       0.03      0.03      0.03        61\n",
      "    positive       0.89      0.89      0.89      1089\n",
      "\n",
      "    accuracy                           0.79      1260\n",
      "   macro avg       0.39      0.38      0.38      1260\n",
      "weighted avg       0.79      0.79      0.79      1260\n",
      "\n",
      "Case3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.19      0.18      0.18       110\n",
      "     neutral       0.10      0.12      0.11        57\n",
      "    positive       0.89      0.88      0.89      1093\n",
      "\n",
      "    accuracy                           0.79      1260\n",
      "   macro avg       0.39      0.40      0.39      1260\n",
      "weighted avg       0.79      0.79      0.79      1260\n",
      "\n",
      "Case4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.16      0.18      0.17        97\n",
      "     neutral       0.09      0.10      0.10        58\n",
      "    positive       0.89      0.88      0.88      1105\n",
      "\n",
      "    accuracy                           0.79      1260\n",
      "   macro avg       0.38      0.38      0.38      1260\n",
      "weighted avg       0.80      0.79      0.79      1260\n",
      "\n",
      "solution for MNB\n",
      "Case1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.09      0.45      0.15        22\n",
      "     neutral       0.03      0.14      0.05        14\n",
      "    positive       0.98      0.87      0.92      1224\n",
      "\n",
      "    accuracy                           0.85      1260\n",
      "   macro avg       0.37      0.49      0.37      1260\n",
      "weighted avg       0.95      0.85      0.90      1260\n",
      "\n",
      "Case2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.39      0.58      0.46        73\n",
      "     neutral       0.07      0.10      0.09        49\n",
      "    positive       0.95      0.91      0.93      1138\n",
      "\n",
      "    accuracy                           0.85      1260\n",
      "   macro avg       0.47      0.53      0.49      1260\n",
      "weighted avg       0.88      0.85      0.87      1260\n",
      "\n",
      "Case3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.18      0.51      0.26        37\n",
      "     neutral       0.01      0.04      0.02        23\n",
      "    positive       0.98      0.88      0.93      1200\n",
      "\n",
      "    accuracy                           0.86      1260\n",
      "   macro avg       0.39      0.48      0.40      1260\n",
      "weighted avg       0.93      0.86      0.89      1260\n",
      "\n",
      "Case4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.02      1.00      0.04         2\n",
      "     neutral       0.00      0.00      0.00         1\n",
      "    positive       1.00      0.86      0.93      1257\n",
      "\n",
      "    accuracy                           0.86      1260\n",
      "   macro avg       0.34      0.62      0.32      1260\n",
      "weighted avg       1.00      0.86      0.92      1260\n",
      "\n",
      "solution for ANN\n",
      "Case1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.25      0.55      0.34        49\n",
      "     neutral       0.03      0.29      0.05         7\n",
      "    positive       0.98      0.89      0.93      1204\n",
      "\n",
      "    accuracy                           0.87      1260\n",
      "   macro avg       0.42      0.57      0.44      1260\n",
      "weighted avg       0.95      0.87      0.90      1260\n",
      "\n",
      "Case2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.34      0.51      0.41        73\n",
      "     neutral       0.09      0.24      0.13        25\n",
      "    positive       0.97      0.90      0.93      1162\n",
      "\n",
      "    accuracy                           0.87      1260\n",
      "   macro avg       0.47      0.55      0.49      1260\n",
      "weighted avg       0.91      0.87      0.89      1260\n",
      "\n",
      "Case3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\steve\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.27      0.51      0.35        57\n",
      "     neutral       0.06      0.33      0.10        12\n",
      "    positive       0.97      0.89      0.93      1191\n",
      "\n",
      "    accuracy                           0.86      1260\n",
      "   macro avg       0.43      0.58      0.46      1260\n",
      "weighted avg       0.93      0.86      0.89      1260\n",
      "\n",
      "Case4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.03      0.19      0.05        16\n",
      "     neutral       0.01      0.08      0.02        13\n",
      "    positive       0.98      0.87      0.92      1231\n",
      "\n",
      "    accuracy                           0.85      1260\n",
      "   macro avg       0.34      0.38      0.33      1260\n",
      "weighted avg       0.96      0.85      0.90      1260\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\steve\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "# TASK 4.3 PRACTICE CODE\n",
    "###############################\n",
    "Q4_combo = [None, 1000, 100, 10]\n",
    "print('solution for DT')\n",
    "for i in range(0, 4):\n",
    "    print('Case' + str(i+1))\n",
    "    x_sparse = limit_attributes(data, Q4_combo[i])\n",
    "    rate_value = data['rating']\n",
    "    value_train, value_test, rate_train, rate_test = train_test_split(x_sparse, rate_value, test_size = 0.4, random_state=1, shuffle = False)\n",
    "\n",
    "    classifier= tree.DecisionTreeClassifier(max_depth = None)\n",
    "    model = classifier.fit(value_train, rate_train)\n",
    "    rate_test_solu = model.predict(value_test)\n",
    "    print(classification_report(rate_test_solu, rate_test))\n",
    "\n",
    "print('solution for MNB')\n",
    "for i in range(0, 4):\n",
    "    print('Case' + str(i +1))\n",
    "    x_sparse = limit_attributes(data, Q4_combo[i])\n",
    "    rate_value = data['rating']\n",
    "    value_train, value_test, rate_train, rate_test = train_test_split(x_sparse, rate_value, test_size = 0.4, random_state=1, shuffle = False)\n",
    "    classifier= MultinomialNB()\n",
    "    model = classifier.fit(value_train, rate_train)\n",
    "    rate_test_solu = model.predict(value_test)\n",
    "    print(classification_report(rate_test_solu, rate_test))\n",
    "\n",
    "print('solution for ANN')\n",
    "for i in range(0, 4):\n",
    "    print('Case' + str(i+1))\n",
    "    x_sparse = limit_attributes(data, Q4_combo[i])\n",
    "    rate_value = data['rating']\n",
    "    value_train, value_test, rate_train, rate_test = train_test_split(x_sparse, rate_value, test_size = 0.4, random_state=1, shuffle = False)\n",
    "    classifier= MLPClassifier(hidden_layer_sizes=(200,))\n",
    "    model = classifier.fit(value_train, rate_train)\n",
    "    rate_test_solu = model.predict(value_test)\n",
    "    print(classification_report(rate_test_solu, rate_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4.3\n",
    "\n",
    "Evaluate the effect each scenario has on the performance for the three models based on the performance metrics. Explain the differences in the performance. Write your answer in the cell below."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. After limiting the number of attributes, the test results of DT, MNB, and ANN are not very good, and the accuracy of the data test is reduced, especially neutral and negative. Among them, the DT reduction effect is the most obvious.\n",
    "\n",
    "2. However, the limit on the number of attributes does not have a significant impact on the overall accuracy of MNB.\n",
    "\n",
    "3. The reason why the MNB has little impact maybe because the calculation method of MNB uses conditional probability. Data calculation adopts a continuous multiplication method, so reducing a small part of the data will not affect its overall accuracy.\n",
    "\n",
    "4. The main reason for the different velocities in the decreasing of Positive, neutral, and negative is due to the different number of sample bases between different reviews. after limiting the attributes, the problem of insufficient sample bases number for neutral and negative becomes more obvious.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4.4\n",
    "\n",
    "Remove all samples with neutral sentiment from the dataset. Train and test all three models on this new dataset, computing the preformance measures as well. Assume that the first 60% of samples constitute the traing set while the remaining 40% of samples constitute the test set. All tokens in the vocabulary should be used as attributes.\n",
    "\n",
    "Place your code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution for DT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.19      0.49      0.28        43\n",
      "    positive       0.98      0.92      0.95      1157\n",
      "\n",
      "    accuracy                           0.91      1200\n",
      "   macro avg       0.59      0.71      0.61      1200\n",
      "weighted avg       0.95      0.91      0.93      1200\n",
      "\n",
      "solution for MNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.12      0.34      0.18        38\n",
      "    positive       0.98      0.92      0.95      1162\n",
      "\n",
      "    accuracy                           0.90      1200\n",
      "   macro avg       0.55      0.63      0.56      1200\n",
      "weighted avg       0.95      0.90      0.92      1200\n",
      "\n",
      "solution for ANN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.25      0.66      0.36        41\n",
      "    positive       0.99      0.93      0.96      1159\n",
      "\n",
      "    accuracy                           0.92      1200\n",
      "   macro avg       0.62      0.79      0.66      1200\n",
      "weighted avg       0.96      0.92      0.94      1200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_no_neutral = data.copy(deep=True)\n",
    "for i in range(0,len(x_no_neutral['rating'])):\n",
    "    comment = x_no_neutral['rating'][i]\n",
    "    if comment == 'neutral':\n",
    "        x_no_neutral.drop(labels=None,axis=0, index=i, columns=None, inplace=True)\n",
    "x_no_neutral.reset_index(drop=True)\n",
    "print('solution for DT')\n",
    "\n",
    "x_sparse = limit_attributes(x_no_neutral, None)\n",
    "\n",
    "rate_value = x_no_neutral['rating']\n",
    "value_train, value_test, rate_train, rate_test = train_test_split(x_sparse, rate_value, test_size = 0.4, shuffle = False)\n",
    "\n",
    "classifier= tree.DecisionTreeClassifier(max_depth = None)\n",
    "model = classifier.fit(value_train, rate_train)\n",
    "rate_test_solu = model.predict(value_test)\n",
    "print(classification_report(rate_test_solu, rate_test))\n",
    "\n",
    "print('solution for MNB')\n",
    "\n",
    "x_sparse = limit_attributes(x_no_neutral, None)\n",
    "\n",
    "rate_value = x_no_neutral['rating']\n",
    "value_train, value_test, rate_train, rate_test = train_test_split(x_sparse, rate_value, test_size = 0.4, shuffle = False)\n",
    "classifier= MultinomialNB()\n",
    "model = classifier.fit(value_train, rate_train)\n",
    "rate_test_solu = model.predict(value_test)\n",
    "print(classification_report(rate_test_solu, rate_test))\n",
    "\n",
    "print('solution for ANN')\n",
    "\n",
    "x_sparse = limit_attributes(x_no_neutral, None)\n",
    "\n",
    "rate_value = x_no_neutral['rating']\n",
    "value_train, value_test, rate_train, rate_test = train_test_split(x_sparse, rate_value, test_size = 0.4, shuffle = False)\n",
    "classifier= MLPClassifier(hidden_layer_sizes=(200,))\n",
    "model = classifier.fit(value_train, rate_train)\n",
    "rate_test_solu = model.predict(value_test)\n",
    "print(classification_report(rate_test_solu, rate_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4.5\n",
    "Compare these results to the results obtained in scenario #1 (part 4). Is there any difference in the metrics for either of the classes (i.e. consider positive and negative classes individually)?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "For both Positive and Negative, the accuracy of the entire project has increased, because after deleting neutral, the proportion of positive and negative sample base number in the total project has increased.\n",
    "\n",
    "Positive: For the three different Classifiers, the accuracy of positive has been increased, especially the effect of Decision Tree is the most obvious. whereas, the improvement of MNB is the smallest.\n",
    "\n",
    "\n",
    "Negative: For the three different Classifiers, the accuracy improvement effect is more significant than positive, especially in MNB.\n",
    "\n",
    "It may be that after neutral is deleted, the proportion of negative sample base has increased, and the two types of different reviews are more polarized, so the learning effect has increased.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 [8 marks]\n",
    "\n",
    "In this part you need to develop your own method (pipeline) for sentiment analysis. You can create new code cells (to place your code) and Raw NBConvert cells (to descibe the steps you are taking to develop your model, or make a comment).\n",
    "\n",
    "You can use either Decision Tree, Multinomial Naive Bayes (MNB), or Artificial Neural Network (ANN) classifiers. If you use parameters for your classifier, the you need to justify the values that you chose for them in writing or in coding & writing (if it is based on evaluation resuts) - similar to Part 2.\n",
    "\n",
    "Since this part of the assignment carries the highest mark, the explanation of your method and its evaluation should be descibed in detail.\n",
    "\n",
    "Optionally: you can also make plots to visualize your findings. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Firstly, I am going to do some preprocess for the data we already have:\n",
    "\n",
    "1. During Part 3 - Pre-processing, we already found that the preprocessing_3 is the most effective method for the improvement of accuracy.\n",
    "    so firstly, I am going to using The DATA after preprocessing_3.\n",
    "\n",
    "\n",
    "2. another preprocessing we can do for the Data is \n",
    "    Stemming words\n",
    "    as there are some words which represent the same meaning but using the different formate \n",
    "    for example, worked, work, and working.\n",
    "    \n",
    "    so after stemming words, all the work would use the same formate, which would help us reduce plenty of meaningless and unuseful repeated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>variation</th>\n",
       "      <th>verified_reviews</th>\n",
       "      <th>feedback</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>love my echo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>love it</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Walnut Finish</td>\n",
       "      <td>sometim while play a game you can answer a que...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>i have had a lot of fun with thi thing my 4 yr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>music</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3145</th>\n",
       "      <td>positive</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Black  Dot</td>\n",
       "      <td>perfect for kid adult and everyon in between</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3146</th>\n",
       "      <td>positive</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Black  Dot</td>\n",
       "      <td>listen to music search locat check time look u...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3147</th>\n",
       "      <td>positive</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Black  Dot</td>\n",
       "      <td>i do love these thing i have them run my entir...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3148</th>\n",
       "      <td>positive</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>White  Dot</td>\n",
       "      <td>onli complaint i have is that the sound qualit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3149</th>\n",
       "      <td>positive</td>\n",
       "      <td>29-Jul-18</td>\n",
       "      <td>Black  Dot</td>\n",
       "      <td>good</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        rating       date         variation  \\\n",
       "0     positive  31-Jul-18  Charcoal Fabric    \n",
       "1     positive  31-Jul-18  Charcoal Fabric    \n",
       "2     positive  31-Jul-18    Walnut Finish    \n",
       "3     positive  31-Jul-18  Charcoal Fabric    \n",
       "4     positive  31-Jul-18  Charcoal Fabric    \n",
       "...        ...        ...               ...   \n",
       "3145  positive  30-Jul-18        Black  Dot   \n",
       "3146  positive  30-Jul-18        Black  Dot   \n",
       "3147  positive  30-Jul-18        Black  Dot   \n",
       "3148  positive  30-Jul-18        White  Dot   \n",
       "3149  positive  29-Jul-18        Black  Dot   \n",
       "\n",
       "                                       verified_reviews  feedback  \n",
       "0                                          love my echo         1  \n",
       "1                                               love it         1  \n",
       "2     sometim while play a game you can answer a que...         1  \n",
       "3     i have had a lot of fun with thi thing my 4 yr...         1  \n",
       "4                                                 music         1  \n",
       "...                                                 ...       ...  \n",
       "3145       perfect for kid adult and everyon in between         1  \n",
       "3146  listen to music search locat check time look u...         1  \n",
       "3147  i do love these thing i have them run my entir...         1  \n",
       "3148  onli complaint i have is that the sound qualit...         1  \n",
       "3149                                               good         1  \n",
       "\n",
       "[3150 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "x_new = preprocessing_3(data)\n",
    "\n",
    "for i in range(0,len(x_new['verified_reviews'])):\n",
    "    comment = x_new['verified_reviews'][i]\n",
    "    comment= comment.split()\n",
    "    \n",
    "    ####################################   \n",
    "    # Stemming words (remove -ing, -ly, ...) \n",
    "    ####################################   \n",
    "    comment_sw = [porter_stemmer.stem(u) for u in comment]\n",
    "    \n",
    "\n",
    "    comment = (\" \").join(comment_sw)\n",
    "    x_new.loc[i,'verified_reviews'] = comment\n",
    "\n",
    "\n",
    "x_new"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3. after stemming words, I found there are some words that are not completed.\n",
    " so we can use WordNetLemmatizer, which should reformate the word, make it be completed but without -s, -ing, -ly.\n",
    "\n",
    " for example, offici -> official"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "x_new = preprocessing_3(data)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0,len(x_new['verified_reviews'])):\n",
    "    comment = x_new['verified_reviews'][i]\n",
    "    comment= comment.split()\n",
    "    ####################################   \n",
    "    # Stemming words (remove -ing, -ly, ...) \n",
    "    ####################################   \n",
    "    comment_sw = [porter_stemmer.stem(u) for u in comment]\n",
    "\n",
    "    ####################################   \n",
    "    # Lemmatisation (convert the word into root word;;;   offici   ->   official)\n",
    "    ####################################\n",
    "    comment_sw = [wordnet_lemmatizer.lemmatize(u) for u in comment_sw]\n",
    "\n",
    "    comment = (\" \").join(comment_sw)\n",
    "    x_new.loc[i,'verified_reviews'] = comment\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Then\n",
    "    4. I am going to use GridSearchCV to test different parameters for\n",
    "    ANN MLPClassifier. \n",
    "    in order to obtain the optimal parameters for MLP.\n",
    "    \n",
    "    5. Based on Task 4.3, here I am going to use '1000' as max_features\n",
    "    \n",
    "    6. Meanwhile, because of the lack ness of the number sample base of neutral and negative\n",
    "    I'm going to set the studying proportion as 80%, test as 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'hidden_layer_sizes': (100, 100, 50), 'max_iter': 20, 'solver': 'adam'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\steve\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "############################################\n",
    "## set max_features up to 1000 \n",
    "############################################\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=lambda x: x.split(), max_features= 1000)\n",
    "x_dense = vectorizer.fit_transform(x_new['verified_reviews'])\n",
    "x_sparse = pd.DataFrame.sparse.from_spmatrix(x_dense, columns=vectorizer.get_feature_names())\n",
    "\n",
    "rate_value = x_new['rating']\n",
    "value_train, value_test, rate_train, rate_test = train_test_split(x_sparse, rate_value, test_size = 0.2,shuffle = False)\n",
    "\n",
    "############################################\n",
    "## ANN parameters possible set\n",
    "############################################\n",
    "\n",
    "ANN_parameters = {\"hidden_layer_sizes\": [(50,50,50,50), (100,100,50)],\n",
    "                             \"activation\":['identity','logistic', 'tanh', 'relu'],\n",
    "                             \"solver\": ['adam', 'sgd', 'lbfgs'],\n",
    "                             \"max_iter\": [20],\n",
    "                             }\n",
    "\n",
    "ANN_CLF = MLPClassifier()\n",
    "estimator = GridSearchCV(ANN_CLF, ANN_parameters, n_jobs= 6)\n",
    "estimator.fit(value_train, rate_train)\n",
    "\n",
    "print (estimator.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Finally,\n",
    "    7. I'm using VotingClassifier to obtain the best classifier for the studying.\n",
    "    and all the parameters used for ANN are from the last GridSearchCV cell.\n",
    "\n",
    "    8. Meanwhile, the max_depth using for DT is 50 which is talked about above in part2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.54      0.79      0.64        33\n",
      "     neutral       0.46      0.52      0.49        23\n",
      "    positive       0.97      0.94      0.96       574\n",
      "\n",
      "    accuracy                           0.92       630\n",
      "   macro avg       0.66      0.75      0.70       630\n",
      "weighted avg       0.93      0.92      0.93       630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "############################################\n",
    "## set three Classifiers for hard voting\n",
    "############################################\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=lambda x: x.split(), max_features= 1000)\n",
    "x_dense = vectorizer.fit_transform(x_new['verified_reviews'])\n",
    "x_sparse = pd.DataFrame.sparse.from_spmatrix(x_dense, columns=vectorizer.get_feature_names())\n",
    "\n",
    "rate_value = x_new['rating']\n",
    "value_train, value_test, rate_train, rate_test = train_test_split(x_sparse, rate_value, test_size = 0.2)\n",
    "\n",
    "clf1 = tree.DecisionTreeClassifier(max_depth = 50)\n",
    "clf2 = MultinomialNB()\n",
    "clf3 = MLPClassifier(activation  = 'relu', hidden_layer_sizes= [50,50,50,50],solver= 'adam', max_iter= 2000)\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('DT',clf1),('MNB',clf2),('ANN',clf3)], voting='hard')\n",
    "\n",
    "    \n",
    "model = classifier.fit(value_train, rate_train)\n",
    "rate_test_solu = model.predict(value_test)\n",
    "print(classification_report(rate_test_solu, rate_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
